{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1: Instalación de dependencias\n",
    "!pip install transformers\n",
    "!pip install tensorflow\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Importaciones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "# Montar Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBERTClassifier:\n",
    "    def __init__(self, max_length=64):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = None\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Construye un modelo BERT para clasificación\"\"\"\n",
    "        # Input layers\n",
    "        input_ids = tf.keras.layers.Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask = tf.keras.layers.Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n",
    "        \n",
    "        # BERT layer con trainable=True\n",
    "        bert = TFBertModel.from_pretrained('bert-base-uncased', trainable=True)\n",
    "        bert_outputs = bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        # Usar el token [CLS] y añadir capas adicionales\n",
    "        cls_token = bert_outputs[:, 0, :]\n",
    "        dropout = tf.keras.layers.Dropout(0.1)(cls_token)\n",
    "        dense1 = tf.keras.layers.Dense(256, activation='relu')(dropout)\n",
    "        dropout2 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "        outputs = tf.keras.layers.Dense(3, activation='softmax')(dropout2)\n",
    "        \n",
    "        # Crear modelo\n",
    "        self.model = tf.keras.Model(\n",
    "            inputs={'input_ids': input_ids, 'attention_mask': attention_mask},\n",
    "            outputs=outputs\n",
    "        )\n",
    "        \n",
    "        # Compilar con optimizador personalizado y learning rate bajo\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def prepare_data(self, data_file, samples_per_class=30):\n",
    "        \"\"\"Prepara una muestra balanceada pequeña de datos\"\"\"\n",
    "        logger.info(\"Iniciando preparación de datos...\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar datos\n",
    "            df = pd.read_csv(data_file)\n",
    "            logger.info(f\"Tamaño original del DataFrame: {len(df)}\")\n",
    "            \n",
    "            # Verificar columnas requeridas\n",
    "            required_columns = ['reviews.text_processed', 'sentiment']\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                raise ValueError(f\"Faltan columnas requeridas: {required_columns}\")\n",
    "            \n",
    "            # Validar valores de sentiment\n",
    "            valid_sentiments = ['negative', 'neutral', 'positive']\n",
    "            if not all(df['sentiment'].isin(valid_sentiments)):\n",
    "                raise ValueError(f\"Valores inválidos en columna 'sentiment'\")\n",
    "            \n",
    "            # Tomar muestra balanceada\n",
    "            balanced_df = pd.DataFrame()\n",
    "            for sentiment in valid_sentiments:\n",
    "                sentiment_samples = df[df['sentiment'] == sentiment].sample(\n",
    "                    n=min(samples_per_class, len(df[df['sentiment'] == sentiment])),\n",
    "                    random_state=42\n",
    "                )\n",
    "                balanced_df = pd.concat([balanced_df, sentiment_samples])\n",
    "            \n",
    "            # Mezclar el DataFrame balanceado\n",
    "            balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            # Preparar textos y etiquetas\n",
    "            texts = balanced_df['reviews.text_processed'].fillna('').astype(str).str[:self.max_length*4].tolist()\n",
    "            sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "            labels = pd.get_dummies(balanced_df['sentiment'].map(sentiment_map)).values\n",
    "            \n",
    "            # Tokenizar con manejo de errores\n",
    "            try:\n",
    "                encodings = self.tokenizer(\n",
    "                    texts,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors='tf'\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en tokenización: {str(e)}\")\n",
    "                raise\n",
    "            \n",
    "            # Convertir a tensores de TensorFlow\n",
    "            input_ids = tf.convert_to_tensor(encodings['input_ids'], dtype=tf.int32)\n",
    "            attention_mask = tf.convert_to_tensor(encodings['attention_mask'], dtype=tf.int32)\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            \n",
    "            # Dividir datos de manera estratificada\n",
    "            total_size = len(texts)\n",
    "            train_size = int(0.8 * total_size)\n",
    "            \n",
    "            # Crear índices aleatorios\n",
    "            indices = tf.range(total_size)\n",
    "            indices = tf.random.shuffle(indices)\n",
    "            \n",
    "            train_indices = indices[:train_size]\n",
    "            val_indices = indices[train_size:]\n",
    "            \n",
    "            return {\n",
    "                'train': {\n",
    "                    'input_ids': tf.gather(input_ids, train_indices),\n",
    "                    'attention_mask': tf.gather(attention_mask, train_indices),\n",
    "                    'labels': tf.gather(labels, train_indices)\n",
    "                },\n",
    "                'val': {\n",
    "                    'input_ids': tf.gather(input_ids, val_indices),\n",
    "                    'attention_mask': tf.gather(attention_mask, val_indices),\n",
    "                    'labels': tf.gather(labels, val_indices)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en prepare_data: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Función principal para entrenamiento del modelo\"\"\"\n",
    "    data_file = \"/content/drive/MyDrive/IronHack/Proyecto4/nlp-project/data/processed/reviews_preprocessed.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Crear clasificador\n",
    "        classifier = SimpleBERTClassifier(max_length=32)\n",
    "        \n",
    "        # Preparar datos balanceados\n",
    "        logger.info(\"Preparando datos...\")\n",
    "        data = classifier.prepare_data(\n",
    "            data_file, \n",
    "            samples_per_class=30\n",
    "        )\n",
    "        \n",
    "        # Construir y entrenar modelo\n",
    "        logger.info(\"Construyendo y entrenando modelo...\")\n",
    "        model = classifier.build_model()\n",
    "        \n",
    "        # Configurar callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=2,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                'best_model.h5',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Entrenar\n",
    "        history = model.fit(\n",
    "            {\n",
    "                'input_ids': data['train']['input_ids'],\n",
    "                'attention_mask': data['train']['attention_mask']\n",
    "            },\n",
    "            data['train']['labels'],\n",
    "            validation_data=(\n",
    "                {\n",
    "                    'input_ids': data['val']['input_ids'],\n",
    "                    'attention_mask': data['val']['attention_mask']\n",
    "                },\n",
    "                data['val']['labels']\n",
    "            ),\n",
    "            epochs=3,\n",
    "            batch_size=8,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Entrenamiento completado\")\n",
    "        return history, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error durante el proceso: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar datos antes de entrenar\n",
    "data_file = \"/content/drive/MyDrive/IronHack/Proyecto4/nlp-project/data/processed/reviews_preprocessed.csv\"\n",
    "if Path(data_file).exists():\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(\"Dimensiones del DataFrame:\", df.shape)\n",
    "    print(\"\\nDistribución de sentiment:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    print(\"\\nMuestra de texto:\")\n",
    "    print(df['reviews.text_processed'].iloc[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6: Ejecutar el entrenamiento\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar las métricas de entrenamiento\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_title('Model accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('Model loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Después del entrenamiento\n",
    "history, model = main()\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
