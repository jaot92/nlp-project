{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTSentimentClassifier:\n",
    "    def __init__(self, max_length=128, learning_rate=2e-5, warmup_steps=0, dropout_rate=0.2):\n",
    "        self.max_length = max_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = None\n",
    "        \n",
    "        # Configuración de dispositivo\n",
    "        self.strategy = self._setup_strategy()\n",
    "        \n",
    "    def _setup_strategy(self):\n",
    "        \"\"\"\n",
    "        Configura la estrategia de entrenamiento basada en el hardware disponible.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Intentar usar TPU si está disponible\n",
    "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.TPUStrategy(tpu)\n",
    "            logger.info(\"Entrenando en TPU\")\n",
    "        except:\n",
    "            # Si no hay TPU, intentar usar GPU\n",
    "            if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "                strategy = tf.distribute.MirroredStrategy()\n",
    "                logger.info(f\"Entrenando en {strategy.num_replicas_in_sync} GPU(s)\")\n",
    "            else:\n",
    "                strategy = tf.distribute.get_strategy()\n",
    "                logger.info(\"Entrenando en CPU\")\n",
    "        \n",
    "        return strategy\n",
    "        \n",
    "    def _validate_input_data(self, df):\n",
    "        \"\"\"\n",
    "        Valida los datos de entrada.\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            raise ValueError(\"El DataFrame está vacío\")\n",
    "            \n",
    "        required_columns = ['reviews.text_processed', 'sentiment']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Columnas faltantes: {missing_columns}\")\n",
    "            \n",
    "        # Verificar valores nulos\n",
    "        null_counts = df[required_columns].isnull().sum()\n",
    "        if null_counts.any():\n",
    "            logger.warning(f\"Valores nulos encontrados:\\n{null_counts}\")\n",
    "            \n",
    "        # Verificar longitud de textos\n",
    "        text_lengths = df['reviews.text_processed'].str.len()\n",
    "        logger.info(f\"Estadísticas de longitud de texto:\\n{text_lengths.describe()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def prepare_data(self, data_file: str):\n",
    "        \"\"\"\n",
    "        Prepara los datos para el entrenamiento con BERT.\n",
    "        \"\"\"\n",
    "        logger.info(\"Cargando y preparando datos...\")\n",
    "        \n",
    "        # Cargar y validar datos\n",
    "        df = pd.read_csv(data_file)\n",
    "        df = self._validate_input_data(df)\n",
    "        \n",
    "        # Limpiar valores nulos\n",
    "        df['reviews.text_processed'] = df['reviews.text_processed'].fillna('')\n",
    "        df = df.dropna(subset=['sentiment'])\n",
    "        \n",
    "        # Tokenizar textos con manejo de errores\n",
    "        try:\n",
    "            encodings = self.tokenizer(\n",
    "                df['reviews.text_processed'].tolist(),\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en la tokenización: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # Preparar etiquetas\n",
    "        y = pd.get_dummies(df['sentiment']).values\n",
    "        \n",
    "        # Calcular class weights\n",
    "        class_counts = df['sentiment'].value_counts()\n",
    "        total = len(df)\n",
    "        class_weights = {\n",
    "            i: total / (len(class_counts) * count)\n",
    "            for i, count in enumerate(class_counts.sort_index())\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Pesos por clase:\\n{class_weights}\")\n",
    "        \n",
    "        # División train/test\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            range(len(df)), test_size=0.2, random_state=42,\n",
    "            stratify=df['sentiment']\n",
    "        )\n",
    "        \n",
    "        # Preparar datos\n",
    "        X_train = {\n",
    "            'input_ids': encodings['input_ids'][train_idx],\n",
    "            'attention_mask': encodings['attention_mask'][train_idx]\n",
    "        }\n",
    "        X_test = {\n",
    "            'input_ids': encodings['input_ids'][test_idx],\n",
    "            'attention_mask': encodings['attention_mask'][test_idx]\n",
    "        }\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        logger.info(f\"Datos divididos - Train: {len(train_idx)}, Test: {len(test_idx)}\")\n",
    "        return X_train, X_test, y_train, y_test, class_weights\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Construye el modelo BERT con capas adicionales para clasificación.\n",
    "        \"\"\"\n",
    "        with self.strategy.scope():\n",
    "            # Cargar modelo base BERT\n",
    "            bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "            \n",
    "            # Inputs\n",
    "            input_ids = tf.keras.layers.Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n",
    "            attention_mask = tf.keras.layers.Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n",
    "            \n",
    "            # BERT layer\n",
    "            bert_outputs = bert(input_ids, attention_mask=attention_mask)[0]\n",
    "            \n",
    "            # Usar el token [CLS] para clasificación\n",
    "            cls_output = bert_outputs[:, 0, :]\n",
    "            \n",
    "            # Capas adicionales con dropout configurable\n",
    "            x = tf.keras.layers.Dense(256, activation='relu')(cls_output)\n",
    "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "            x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "            outputs = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "            \n",
    "            # Construir modelo\n",
    "            self.model = tf.keras.Model(\n",
    "                inputs=[input_ids, attention_mask],\n",
    "                outputs=outputs\n",
    "            )\n",
    "            \n",
    "            # Learning rate schedule con warmup\n",
    "            total_steps = 1000  # Ajustar según el tamaño del dataset\n",
    "            warmup_steps = self.warmup_steps\n",
    "            \n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=self.learning_rate,\n",
    "                decay_steps=total_steps - warmup_steps,\n",
    "                end_learning_rate=self.learning_rate * 0.1\n",
    "            )\n",
    "            \n",
    "            if warmup_steps:\n",
    "                lr_schedule = WarmUp(\n",
    "                    initial_learning_rate=self.learning_rate,\n",
    "                    decay_schedule_fn=lr_schedule,\n",
    "                    warmup_steps=warmup_steps\n",
    "                )\n",
    "            \n",
    "            # Compilar modelo con gradient clipping\n",
    "            optimizer = tf.keras.optimizers.Adam(\n",
    "                learning_rate=lr_schedule,\n",
    "                clipnorm=1.0\n",
    "            )\n",
    "            \n",
    "            self.model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Resumen del modelo:\")\n",
    "            logger.info(self.model.summary())\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, class_weights, epochs=5, batch_size=32):\n",
    "        \"\"\"\n",
    "        Entrena el modelo BERT con validación cruzada.\n",
    "        \"\"\"\n",
    "        logger.info(\"Iniciando entrenamiento del modelo BERT...\")\n",
    "        \n",
    "        # Configurar callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=2,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=1,\n",
    "                min_lr=1e-7\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir='./logs',\n",
    "                histogram_freq=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Entrenamiento principal\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            class_weight=class_weights,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Validación cruzada\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train['input_ids'])):\n",
    "            logger.info(f\"Entrenando fold {fold + 1}/5\")\n",
    "            \n",
    "            # Preparar datos para este fold\n",
    "            X_train_fold = {\n",
    "                'input_ids': X_train['input_ids'][train_idx],\n",
    "                'attention_mask': X_train['attention_mask'][train_idx]\n",
    "            }\n",
    "            X_val_fold = {\n",
    "                'input_ids': X_train['input_ids'][val_idx],\n",
    "                'attention_mask': X_train['attention_mask'][val_idx]\n",
    "            }\n",
    "            y_train_fold = y_train[train_idx]\n",
    "            y_val_fold = y_train[val_idx]\n",
    "            \n",
    "            # Entrenar en este fold\n",
    "            self.build_model()  # Reiniciar modelo para cada fold\n",
    "            history_fold = self.model.fit(\n",
    "                X_train_fold,\n",
    "                y_train_fold,\n",
    "                epochs=2,  # Menos épocas para CV\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(X_val_fold, y_val_fold),\n",
    "                class_weight=class_weights,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Guardar score\n",
    "            cv_scores.append(history_fold.history['val_accuracy'][-1])\n",
    "        \n",
    "        logger.info(f\"Scores de validación cruzada: {cv_scores}\")\n",
    "        logger.info(f\"Media CV: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, output_dir: str):\n",
    "        \"\"\"\n",
    "        Evalúa el modelo y guarda los resultados.\n",
    "        \"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_test_classes = np.argmax(y_test, axis=1)\n",
    "        \n",
    "        # Reporte de clasificación\n",
    "        report = classification_report(y_test_classes, y_pred_classes)\n",
    "        logger.info(\"\\nClassification Report:\")\n",
    "        logger.info(f\"\\n{report}\")\n",
    "        \n",
    "        # Matriz de confusión\n",
    "        cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "        \n",
    "        # Visualizaciones\n",
    "        self._plot_confusion_matrix(cm, output_path)\n",
    "        self._plot_roc_curves(y_test, y_pred, output_path)\n",
    "        self._plot_precision_recall_curves(y_test, y_pred, output_path)\n",
    "        \n",
    "        # Guardar modelo y tokenizer\n",
    "        self.model.save_pretrained(output_path / \"bert_model\")\n",
    "        self.tokenizer.save_pretrained(output_path / \"bert_tokenizer\")\n",
    "        \n",
    "        # Guardar métricas en formato JSON\n",
    "        metrics = {\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm.tolist()\n",
    "        }\n",
    "        with open(output_path / 'metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        \n",
    "        return report, cm\n",
    "    \n",
    "    def _plot_confusion_matrix(self, cm, output_path):\n",
    "        \"\"\"\n",
    "        Visualiza y guarda la matriz de confusión.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Matriz de Confusión - BERT')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig(output_path / \"bert_confusion_matrix.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_roc_curves(self, y_test, y_pred, output_path):\n",
    "        \"\"\"\n",
    "        Genera y guarda las curvas ROC.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for i in range(y_test.shape[1]):\n",
    "            fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'Clase {i} (AUC = {roc_auc:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Curvas ROC por Clase')\n",
    "        plt.legend()\n",
    "        plt.savefig(output_path / \"bert_roc_curves.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_precision_recall_curves(self, y_test, y_pred, output_path):\n",
    "        \"\"\"\n",
    "        Genera y guarda las curvas de Precision-Recall.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for i in range(y_test.shape[1]):\n",
    "            precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n",
    "            plt.plot(recall, precision, label=f'Clase {i}')\n",
    "        \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Curvas Precision-Recall por Clase')\n",
    "        plt.legend()\n",
    "        plt.savefig(output_path / \"bert_precision_recall_curves.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Implementación de warmup para learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, decay_schedule_fn, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(\"learning_rate\"):\n",
    "            global_step_float = tf.cast(step, tf.float32)\n",
    "            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "            \n",
    "            warmup_percent_done = global_step_float / warmup_steps_float\n",
    "            warmup_learning_rate = self.initial_learning_rate * warmup_percent_done\n",
    "            \n",
    "            return tf.cond(\n",
    "                global_step_float < warmup_steps_float,\n",
    "                lambda: warmup_learning_rate,\n",
    "                lambda: self.decay_schedule_fn(step - self.warmup_steps)\n",
    "            )\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"decay_schedule_fn\": self.decay_schedule_fn,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 13:20:31,888 - __main__ - INFO - Iniciando entrenamiento del modelo BERT...\n",
      "2025-01-22 13:20:32,044 - __main__ - INFO - Entrenando en CPU\n",
      "2025-01-22 13:20:32,045 - __main__ - INFO - Cargando y preparando datos...\n",
      "2025-01-22 13:20:32,320 - __main__ - WARNING - Valores nulos encontrados:\n",
      "reviews.text_processed    26\n",
      "sentiment                  0\n",
      "dtype: int64\n",
      "2025-01-22 13:20:32,331 - __main__ - INFO - Estadísticas de longitud de texto:\n",
      "count    67966.000000\n",
      "mean        91.774578\n",
      "std        120.029156\n",
      "min          1.000000\n",
      "25%         40.000000\n",
      "50%         60.000000\n",
      "75%        106.000000\n",
      "max       7048.000000\n",
      "Name: reviews.text_processed, dtype: float64\n",
      "2025-01-22 13:20:44,924 - __main__ - INFO - Pesos por clase:\n",
      "{0: 8.91230829728667, 1: 7.809786354238456, 2: 0.36235151166322926}\n",
      "2025-01-22 13:20:45.013104: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [54393], [54393], and [54393] instead.\n",
      "2025-01-22 13:20:45,013 - __main__ - ERROR - Error durante el entrenamiento: int too big to convert\n",
      "2025-01-22 13:20:45.013117: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [54393], [54393], and [54393] instead.\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "int too big to convert",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zf/26z9dj9s4rgb9txx_1j9fkv80000gn/T/ipykernel_75360/2696596649.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;34m\"\"\"Función principal para ejecutar el entrenamiento del modelo BERT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/processed/reviews_preprocessed.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../models/deep_learning\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zf/26z9dj9s4rgb9txx_1j9fkv80000gn/T/ipykernel_75360/2696596649.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entrenamiento del modelo BERT completado exitosamente.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error durante el entrenamiento: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/zf/26z9dj9s4rgb9txx_1j9fkv80000gn/T/ipykernel_75360/3182422206.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data_file)\u001b[0m\n\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Preparar datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         X_train = {\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         }\n\u001b[1;32m    109\u001b[0m         X_test = {\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlp-reviews/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/nlp-reviews/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  11011\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11012\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11013\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11014\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11015\u001b[0;31m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11016\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11017\u001b[0m       return strided_slice_eager_fallback(\n\u001b[1;32m  11018\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: int too big to convert"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Función principal para ejecutar el entrenamiento del modelo BERT.\"\"\"\n",
    "    data_file = \"../data/processed/reviews_preprocessed.csv\"\n",
    "    output_dir = \"../models/deep_learning\"\n",
    "    \n",
    "    logger.info(\"Iniciando entrenamiento del modelo BERT...\")\n",
    "    \n",
    "    try:\n",
    "        # Inicializar clasificador\n",
    "        classifier = BERTSentimentClassifier()\n",
    "        \n",
    "        # Preparar datos\n",
    "        X_train, X_test, y_train, y_test, class_weights = classifier.prepare_data(data_file)\n",
    "        \n",
    "        # Construir modelo\n",
    "        classifier.build_model()\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        history = classifier.train(X_train, y_train, X_test, y_test, class_weights)\n",
    "        \n",
    "        # Evaluar y guardar resultados\n",
    "        report, cm = classifier.evaluate(X_test, y_test, output_dir)\n",
    "        \n",
    "        logger.info(\"Entrenamiento del modelo BERT completado exitosamente.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error durante el entrenamiento: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-reviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
