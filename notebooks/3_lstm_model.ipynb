{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import joblib\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMSentimentClassifier:\n",
    "    \"\"\"\n",
    "    Clase para entrenar y evaluar un modelo LSTM bidireccional para clasificación de sentimiento.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_words=10000, max_len=200, embedding_dim=100):\n",
    "        \"\"\"\n",
    "        Inicializa el clasificador LSTM.\n",
    "        \n",
    "        Args:\n",
    "            max_words (int): Número máximo de palabras en el vocabulario\n",
    "            max_len (int): Longitud máxima de las secuencias\n",
    "            embedding_dim (int): Dimensión del embedding\n",
    "        \"\"\"\n",
    "        self.max_words = max_words\n",
    "        self.max_len = max_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tokenizer = Tokenizer(num_words=max_words)\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_data(self, data_file: str):\n",
    "        \"\"\"\n",
    "        Prepara los datos para el entrenamiento.\n",
    "        \n",
    "        Args:\n",
    "            data_file (str): Ruta al archivo de datos preprocesados\n",
    "        \n",
    "        Returns:\n",
    "            tuple: X_train, X_test, y_train, y_test, class_weights\n",
    "        \"\"\"\n",
    "        logger.info(\"Cargando y preparando datos...\")\n",
    "        \n",
    "        # Cargar datos\n",
    "        df = pd.read_csv(data_file)\n",
    "        \n",
    "        # Verificar y reportar valores nulos\n",
    "        null_counts = df[['reviews.text_processed', 'sentiment']].isnull().sum()\n",
    "        logger.info(f\"Valores nulos antes de limpieza:\\n{null_counts}\")\n",
    "        \n",
    "        # Limpiar valores nulos\n",
    "        df['reviews.text_processed'] = df['reviews.text_processed'].fillna('')\n",
    "        df = df.dropna(subset=['sentiment'])\n",
    "        \n",
    "        # Verificar datos después de limpieza\n",
    "        logger.info(f\"Registros después de limpieza: {len(df)}\")\n",
    "        \n",
    "        # Tokenizar textos\n",
    "        self.tokenizer.fit_on_texts(df['reviews.text_processed'])\n",
    "        sequences = self.tokenizer.texts_to_sequences(df['reviews.text_processed'])\n",
    "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
    "        \n",
    "        # Preparar etiquetas\n",
    "        y = pd.get_dummies(df['sentiment']).values\n",
    "        \n",
    "        # Calcular class weights para balancear el entrenamiento\n",
    "        class_counts = df['sentiment'].value_counts()\n",
    "        total = len(df)\n",
    "        base_weights = {\n",
    "            i: total / (len(class_counts) * count)\n",
    "            for i, count in enumerate(class_counts.sort_index())\n",
    "        }\n",
    "        \n",
    "        # Ajustar pesos para dar más énfasis a clases minoritarias\n",
    "        max_weight = max(base_weights.values())\n",
    "        class_weights = {\n",
    "            k: min(v * 2.0, max_weight * 5.0)  # Más énfasis en clases minoritarias\n",
    "            for k, v in base_weights.items()\n",
    "        }\n",
    "        logger.info(f\"\\nPesos por clase:\\n{class_weights}\")\n",
    "        \n",
    "        # División train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=df['sentiment']\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Datos divididos - Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "        return X_train, X_test, y_train, y_test, class_weights\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Construye el modelo LSTM bidireccional con arquitectura mejorada.\n",
    "        \"\"\"\n",
    "        # Regularizadores\n",
    "        lstm_regularizer = l1_l2(l1=1e-5, l2=1e-4)\n",
    "        dense_regularizer = l1_l2(l1=1e-6, l2=1e-5)\n",
    "        \n",
    "        self.model = Sequential([\n",
    "            Embedding(self.max_words, self.embedding_dim, input_length=self.max_len),\n",
    "            Bidirectional(LSTM(256, return_sequences=True, \n",
    "                             kernel_regularizer=lstm_regularizer)),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(128, return_sequences=True,\n",
    "                             kernel_regularizer=lstm_regularizer)),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(64, kernel_regularizer=lstm_regularizer)),\n",
    "            Dropout(0.2),\n",
    "            Dense(256, activation='relu', kernel_regularizer=dense_regularizer),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(128, activation='relu', kernel_regularizer=dense_regularizer),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Construir el modelo con shape de entrada conocido\n",
    "        self.model.build((None, self.max_len))\n",
    "        \n",
    "        # Usar Adam con learning rate más alto y clipnorm\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.001,\n",
    "            clipnorm=1.0\n",
    "        )\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Resumen del modelo:\")\n",
    "        logger.info(self.model.summary())\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, class_weights, epochs=20, batch_size=32):\n",
    "        \"\"\"\n",
    "        Entrena el modelo LSTM con class weights ajustados.\n",
    "        \"\"\"\n",
    "        logger.info(\"Iniciando entrenamiento del modelo LSTM...\")\n",
    "        \n",
    "        # Ajustar class weights para ser menos agresivos\n",
    "        adjusted_weights = {\n",
    "            k: min(v, 3.0) for k, v in class_weights.items()\n",
    "        }\n",
    "        logger.info(f\"Class weights ajustados: {adjusted_weights}\")\n",
    "        \n",
    "        # Callbacks mejorados\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',  # Cambiado a accuracy\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,  # Reducción más suave\n",
    "                patience=3,\n",
    "                min_lr=1e-6\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                'best_model.keras',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            class_weight=adjusted_weights,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, output_dir: str):\n",
    "        \"\"\"\n",
    "        Evalúa el modelo y guarda los resultados.\n",
    "        \n",
    "        Args:\n",
    "            X_test, y_test: Datos de prueba\n",
    "            output_dir (str): Directorio para guardar resultados\n",
    "        \"\"\"\n",
    "        # Crear directorio si no existe\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_test_classes = np.argmax(y_test, axis=1)\n",
    "        \n",
    "        # Reporte de clasificación\n",
    "        report = classification_report(y_test_classes, y_pred_classes, zero_division=1)\n",
    "        logger.info(\"\\nClassification Report:\")\n",
    "        logger.info(f\"\\n{report}\")\n",
    "        \n",
    "        # Matriz de confusión\n",
    "        cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "        \n",
    "        # Visualizar y guardar matriz de confusión\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Matriz de Confusión - LSTM')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig(output_path / \"lstm_confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Guardar modelo y tokenizer\n",
    "        self.model.save(output_path / \"lstm_model.keras\")\n",
    "        joblib.dump(self.tokenizer, output_path / \"lstm_tokenizer.joblib\")\n",
    "        \n",
    "        return report, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Función principal para ejecutar el entrenamiento del modelo LSTM.\"\"\"\n",
    "    # Configuración\n",
    "    data_file = \"../data/processed/reviews_preprocessed.csv\"\n",
    "    output_dir = \"../models/deep_learning\"\n",
    "    \n",
    "    logger.info(\"Iniciando entrenamiento del modelo LSTM...\")\n",
    "    \n",
    "    try:\n",
    "        # Inicializar clasificador\n",
    "        classifier = LSTMSentimentClassifier()\n",
    "        \n",
    "        # Preparar datos\n",
    "        X_train, X_test, y_train, y_test, class_weights = classifier.prepare_data(data_file)\n",
    "        \n",
    "        # Construir modelo\n",
    "        classifier.build_model()\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        history = classifier.train(X_train, y_train, X_test, y_test, class_weights)\n",
    "        \n",
    "        # Evaluar y guardar resultados\n",
    "        report, cm = classifier.evaluate(X_test, y_test, output_dir)\n",
    "        \n",
    "        logger.info(\"\\nEntrenamiento del modelo LSTM completado exitosamente.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error durante el entrenamiento: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
